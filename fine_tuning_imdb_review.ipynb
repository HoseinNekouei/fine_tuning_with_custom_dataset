{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMoeEkszozVpLN82/ZtAFt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoseinNekouei/fine_tuning_with_custom_dataset/blob/main/fine_tuning_imdb_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "EQw2NyjwJYR2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7YBDiiuE-Cb",
        "outputId": "45a4964c-d900-452a-f710-561ba9f6988f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-24 21:13:07--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M   516KB/s    in 81s     \n",
            "\n",
            "2025-02-24 21:14:29 (1010 KB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the dataset and divide into training and testing set**"
      ],
      "metadata": {
        "id": "gowEYYlAJW3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from posixpath import join\n",
        "from pathlib import Path\n",
        "\n",
        "def read_imdb_split(split_dir):\n",
        "  split_dir= Path(split_dir)\n",
        "  texts=[]\n",
        "  labels=[]\n",
        "\n",
        "  for label_dir in ['neg', 'pos']:\n",
        "    for text_file in (split_dir/label_dir).iterdir():\n",
        "\n",
        "      texts.append(text_file.read_text())\n",
        "      labels.append(0 if label_dir== 'neg' else 1)\n",
        "\n",
        "  return texts, labels\n",
        "\n",
        "train_texts, train_labels= read_imdb_split('/content/aclImdb/train')\n",
        "# test_texts, test_labels= read_imdb_split('/content/aclImdb/test')\n",
        "\n",
        "shape=','.join(\n",
        "    [f'train text length: {len(train_texts)}',\n",
        "    f'train labels length: {len(train_labels)}',\n",
        "    # f'test text length: {len(test_texts)}',\n",
        "    # f'test labels length: {len(test_labels)}'\n",
        "    ])\n",
        "\n",
        "print(shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtvE82NrGAp5",
        "outputId": "e4d73426-1f76-4221-970b-c5b6e0c31fbc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train text length: 25000,train labels length: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_texts, train_labels, test_size= 0.2, stratify= train_labels)\n",
        "\n",
        "length= ','. join([\n",
        "    f'X_train length: {len(X_train)}',\n",
        "    f'X_val length: {len(X_val)}',\n",
        "    f'y_train length: {len(y_train)}',\n",
        "    f'y_val length: {len(y_val)}'\n",
        "])\n",
        "\n",
        "print(length)"
      ],
      "metadata": {
        "id": "uFplsiC2JPHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d7f8470-e616-42cb-b0c5-86535eabacd3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train length: 20000,X_val length: 5000,y_train length: 20000,y_val length: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Tokenizer**"
      ],
      "metadata": {
        "id": "RGCp87YjWcDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkF1ZoWnWfHD",
        "outputId": "5ccf44db-1fb9-4b88-cb56-d09108b9e8ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(X_train, padding=True, truncation=True, max_length= 256)\n",
        "val_encodings= tokenizer(X_val, padding= True, truncation= True, max_length= 256)\n",
        "# test_encodings= tokenizer(test_texts, padding= True, truncation= True, max_length= 256)"
      ],
      "metadata": {
        "id": "b4GVdYZyWyJB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Make Dataset and DataLoader**"
      ],
      "metadata": {
        "id": "6-tDsfgMeR-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "W-SfaC-BeAnM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings, labels):\n",
        "    self.encodings= encodings\n",
        "    self.labels= labels\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item= {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    item['labels'] = torch.tensor(self.labels[idx])\n",
        "    return item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "\n",
        "train_set= IMDBDataset(train_encodings, y_train)\n",
        "val_set= IMDBDataset(val_encodings, y_val)"
      ],
      "metadata": {
        "id": "K-R68eqbc7Ng"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=128)"
      ],
      "metadata": {
        "id": "q1oq-5Wkp9qd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "VdW420ZP5s-m",
        "outputId": "4d0ccd9c-86dd-46ce-ddb0-9a152484ed6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch =next(iter(train_loader))\n",
        "# input_ids= batch['input_ids'].to(device)\n",
        "# attention_mask = batch['attention_mask'].to(device)\n",
        "# labels= batch['labels'].to(device)\n",
        "\n",
        "# outputs= model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "# outputs"
      ],
      "metadata": {
        "id": "9M3uYvmS6x6o",
        "outputId": "6774cb7a-0fbd-4c29-eb3b-d1a611c62edd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1128, -0.1132],\n",
              "        [-0.1356, -0.1615],\n",
              "        [ 0.0383, -0.1810],\n",
              "        [-0.1661, -0.0480],\n",
              "        [-0.0782,  0.0043],\n",
              "        [-0.0521, -0.0580],\n",
              "        [-0.1768, -0.1167],\n",
              "        [-0.1440, -0.0950],\n",
              "        [-0.0702, -0.1261],\n",
              "        [-0.0564, -0.0578],\n",
              "        [-0.0533, -0.1006],\n",
              "        [-0.0164, -0.0585],\n",
              "        [-0.0626,  0.0232],\n",
              "        [-0.1222, -0.0421],\n",
              "        [-0.0837, -0.0438],\n",
              "        [-0.0859, -0.0029],\n",
              "        [-0.0714, -0.1400],\n",
              "        [-0.1546, -0.1163],\n",
              "        [-0.1441, -0.1007],\n",
              "        [-0.0765, -0.1121],\n",
              "        [-0.0822,  0.0657],\n",
              "        [-0.0941, -0.0964],\n",
              "        [-0.1562, -0.0719],\n",
              "        [-0.0251, -0.0708],\n",
              "        [ 0.0236,  0.0338],\n",
              "        [-0.1576, -0.1046],\n",
              "        [-0.0491, -0.1092],\n",
              "        [-0.0889, -0.1572],\n",
              "        [-0.0522, -0.1532],\n",
              "        [-0.0007,  0.0083],\n",
              "        [-0.0774,  0.0069],\n",
              "        [-0.1085, -0.0457],\n",
              "        [-0.0373, -0.1111],\n",
              "        [-0.0224, -0.1170],\n",
              "        [ 0.0201, -0.0494],\n",
              "        [-0.0650, -0.1088],\n",
              "        [-0.0338, -0.0943],\n",
              "        [-0.0494, -0.0101],\n",
              "        [-0.0563, -0.0585],\n",
              "        [ 0.0532,  0.0021],\n",
              "        [-0.1350, -0.0782],\n",
              "        [-0.0653,  0.0057],\n",
              "        [ 0.0194, -0.0501],\n",
              "        [-0.0203, -0.0265],\n",
              "        [-0.1031, -0.0523],\n",
              "        [-0.1044, -0.1557],\n",
              "        [-0.1148, -0.0753],\n",
              "        [-0.0161, -0.1011],\n",
              "        [-0.0813, -0.1218],\n",
              "        [-0.0913, -0.1667],\n",
              "        [ 0.0064, -0.0931],\n",
              "        [-0.1091, -0.0335],\n",
              "        [-0.1153, -0.0839],\n",
              "        [-0.0112, -0.1169],\n",
              "        [-0.0735, -0.0929],\n",
              "        [-0.1277, -0.1091],\n",
              "        [-0.0632, -0.0330],\n",
              "        [ 0.0710,  0.0132],\n",
              "        [-0.0141, -0.0935],\n",
              "        [-0.0441,  0.0772],\n",
              "        [-0.0043,  0.0043],\n",
              "        [-0.1544, -0.1047],\n",
              "        [-0.1507, -0.2136],\n",
              "        [-0.0154, -0.0457],\n",
              "        [-0.0331, -0.0463],\n",
              "        [-0.1529, -0.0506],\n",
              "        [-0.1760, -0.0884],\n",
              "        [ 0.0097, -0.1101],\n",
              "        [-0.0163, -0.0530],\n",
              "        [-0.0767,  0.0189],\n",
              "        [-0.0923, -0.0964],\n",
              "        [ 0.0050, -0.1463],\n",
              "        [-0.0813, -0.0610],\n",
              "        [-0.0154,  0.0195],\n",
              "        [-0.0106, -0.0608],\n",
              "        [-0.1109, -0.1707],\n",
              "        [ 0.0103, -0.0856],\n",
              "        [-0.1228, -0.0922],\n",
              "        [-0.0056, -0.0504],\n",
              "        [-0.1034, -0.0284],\n",
              "        [-0.0705, -0.0855],\n",
              "        [-0.0990, -0.1554],\n",
              "        [-0.1675, -0.0755],\n",
              "        [-0.1514, -0.0219],\n",
              "        [-0.0920, -0.1454],\n",
              "        [-0.0233, -0.0461],\n",
              "        [-0.0861, -0.0697],\n",
              "        [-0.1081, -0.1206],\n",
              "        [-0.1322, -0.0589],\n",
              "        [-0.0603, -0.2586],\n",
              "        [ 0.0076, -0.0121],\n",
              "        [-0.0916, -0.1126],\n",
              "        [-0.1172, -0.1677],\n",
              "        [-0.0277, -0.0975],\n",
              "        [-0.0571, -0.0454],\n",
              "        [-0.0237, -0.1132],\n",
              "        [-0.0890, -0.1398],\n",
              "        [-0.0798, -0.1050],\n",
              "        [-0.0611, -0.1212],\n",
              "        [-0.0435, -0.0972],\n",
              "        [ 0.0188, -0.0158],\n",
              "        [-0.0798, -0.2120],\n",
              "        [-0.0118, -0.1007],\n",
              "        [-0.0762, -0.0217],\n",
              "        [-0.0411, -0.0378],\n",
              "        [-0.0993, -0.1020],\n",
              "        [-0.1472, -0.0138],\n",
              "        [-0.0869, -0.0184],\n",
              "        [-0.1272, -0.0623],\n",
              "        [ 0.0657, -0.0613],\n",
              "        [-0.0771, -0.0565],\n",
              "        [-0.0710, -0.0889],\n",
              "        [-0.0487, -0.1148],\n",
              "        [-0.2357, -0.0515],\n",
              "        [-0.0214, -0.0448],\n",
              "        [ 0.0263, -0.0746],\n",
              "        [-0.0376, -0.0397],\n",
              "        [-0.1599, -0.0320],\n",
              "        [-0.0272, -0.0657],\n",
              "        [-0.0764, -0.1538],\n",
              "        [-0.0270, -0.1227],\n",
              "        [-0.0103, -0.0591],\n",
              "        [-0.1610, -0.0628],\n",
              "        [-0.0812, -0.0857],\n",
              "        [-0.0476, -0.0408],\n",
              "        [-0.0286, -0.0626],\n",
              "        [-0.0423, -0.0929],\n",
              "        [-0.0255, -0.0975]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2):\n",
        "  total_loss, train_loss, val_loss = 0, 0, 0\n",
        "\n",
        "  for index, batch in enumerate(train_loader):\n",
        "\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "    loss = outputs[0]\n",
        "    loss.backward()\n",
        "\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "    total_loss += loss.item() * len(batch)\n",
        "\n",
        "    if (index +1) % 10 == 0:\n",
        "      print(f'index no. {index + 1}/{len(train_set)/128}')\n",
        "\n",
        "  train_loss /= len(train_set)\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "      loss= outputs[0]\n",
        "\n",
        "      total_loss += loss.item() * len(batch)\n",
        "\n",
        "    val_loss /= len(val_set)\n",
        "\n",
        "  print(f'Epoch:[{epoch + 1}], '\n",
        "        f'train_Loss:{train_loss:.3f}, '\n",
        "        f'val_Loss:{val_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efN0NPfSm6Y3",
        "outputId": "25c2491e-6c13-42a0-c866-b397b0f3dfbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index no. 10/156.25\n",
            "index no. 20/156.25\n",
            "index no. 30/156.25\n",
            "index no. 40/156.25\n",
            "index no. 50/156.25\n",
            "index no. 60/156.25\n",
            "index no. 70/156.25\n",
            "index no. 80/156.25\n",
            "index no. 90/156.25\n",
            "index no. 100/156.25\n",
            "index no. 110/156.25\n",
            "index no. 120/156.25\n",
            "index no. 130/156.25\n",
            "index no. 140/156.25\n",
            "index no. 150/156.25\n",
            "Epoch:[1] train_Loss: 0.000val_Loss: 0.000\n",
            "index no. 10/156.25\n",
            "index no. 20/156.25\n",
            "index no. 30/156.25\n",
            "index no. 40/156.25\n",
            "index no. 50/156.25\n",
            "index no. 60/156.25\n",
            "index no. 70/156.25\n",
            "index no. 80/156.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     outputs = model(input_ids, attention_mask=attention_mask)\n",
        "#     predictions = torch.argmax(outputs.logits, dim=-1)"
      ],
      "metadata": {
        "id": "9GPcA07AvaCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v-AX1zFkCBh-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}