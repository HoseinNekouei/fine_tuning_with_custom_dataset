{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIyE4BBsRE9NgdTzklIy3M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoseinNekouei/fine_tuning_with_custom_dataset/blob/main/fine_tuning_imdb_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "EQw2NyjwJYR2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7YBDiiuE-Cb",
        "outputId": "a4344f03-a818-43fc-ef03-bb39855b6284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-24 18:31:19--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  4.61MB/s    in 20s     \n",
            "\n",
            "2025-02-24 18:31:40 (3.94 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the dataset and divide into training and testing set**"
      ],
      "metadata": {
        "id": "gowEYYlAJW3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from posixpath import join\n",
        "from pathlib import Path\n",
        "\n",
        "def read_imdb_split(split_dir):\n",
        "  split_dir= Path(split_dir)\n",
        "  texts=[]\n",
        "  labels=[]\n",
        "\n",
        "  for label_dir in ['neg', 'pos']:\n",
        "    for text_file in (split_dir/label_dir).iterdir():\n",
        "\n",
        "      texts.append(text_file.read_text())\n",
        "      labels.append(0 if label_dir== 'neg' else 1)\n",
        "\n",
        "  return texts, labels\n",
        "\n",
        "train_texts, train_labels= read_imdb_split('/content/aclImdb/train')\n",
        "test_texts, test_labels= read_imdb_split('/content/aclImdb/test')\n",
        "\n",
        "shape=','.join(\n",
        "    [f'train text length: {len(train_texts)}',\n",
        "    f'train labels length: {len(train_labels)}',\n",
        "    f'test text length: {len(test_texts)}',\n",
        "    f'test labels length: {len(test_labels)}'\n",
        "    ])\n",
        "\n",
        "print(shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtvE82NrGAp5",
        "outputId": "e2303a40-2f1e-42a5-c8ce-93c1b499d3f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train text length: 25000, train labels length: 25000\n",
            "test text length: 25000, test labels length: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_texts, train_labels, test_size= 0.2, stratify= train_labels)\n",
        "\n",
        "length= ','. join([\n",
        "    f'X_train length: {len(X_train)}',\n",
        "    f'X_val length: {len(X_val)}',\n",
        "    f'y_train length: {len(y_train)}',\n",
        "    f'y_val length: {len(y_val)}'\n",
        "])\n",
        "\n",
        "print(length)"
      ],
      "metadata": {
        "id": "uFplsiC2JPHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def4a9f2-36f0-4436-d7c0-64266884af3e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train length: 20000,X_val length: 5000,y_train length: 20000,y_val length: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Tokenizer**"
      ],
      "metadata": {
        "id": "RGCp87YjWcDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer"
      ],
      "metadata": {
        "id": "nkF1ZoWnWfHD",
        "outputId": "0ca2a127-2b07-47b6-c61b-7a88c755e986",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(X_train, padding=True, truncation=True, max_length= 256, return_tensors= 'pt')\n",
        "val_encodings= tokenizer(X_val, padding= True, truncation= True, max_length= 256, return_tensors= 'pt')\n",
        "test_encodings= tokenizer(test_texts, padding= True, truncation= True, max_length= 256, return_tensors= 'pt')"
      ],
      "metadata": {
        "id": "b4GVdYZyWyJB"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Make Dataset and DataLoader**"
      ],
      "metadata": {
        "id": "6-tDsfgMeR-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "W-SfaC-BeAnM"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings, labels):\n",
        "    self.encodings= encodings\n",
        "    self.labels= labels\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item= {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    item['labels'] = torch.tensor[self.labels[idx]]\n",
        "    return item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "\n",
        "train_set= IMDBDataset(train_encodings, y_train)\n",
        "val_set= IMDBDataset(val_encodings, y_val)\n",
        "test_set= IMDBDataset(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "K-R68eqbc7Ng"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "efN0NPfSm6Y3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}