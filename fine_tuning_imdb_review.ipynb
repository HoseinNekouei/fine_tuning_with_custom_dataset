{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoseinNekouei/fine_tuning_with_custom_dataset/blob/main/fine_tuning_imdb_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "EQw2NyjwJYR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ],
      "metadata": {
        "id": "iqV4PFFekvd_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7YBDiiuE-Cb",
        "outputId": "d7d08c96-0a6f-4273-c80c-f72393d377c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-27 22:43:26--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.1’\n",
            "\n",
            "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  8.04MB/s    in 8.8s    \n",
            "\n",
            "2025-02-27 22:43:35 (9.08 MB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the dataset and divide into training and testing set**"
      ],
      "metadata": {
        "id": "gowEYYlAJW3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from posixpath import join\n",
        "from pathlib import Path\n",
        "\n",
        "def read_imdb_split(split_dir):\n",
        "  split_dir= Path(split_dir)\n",
        "  texts=[]\n",
        "  labels=[]\n",
        "\n",
        "  for label_dir in ['neg', 'pos']:\n",
        "    for text_file in (split_dir/label_dir).iterdir():\n",
        "\n",
        "      texts.append(text_file.read_text())\n",
        "      labels.append(0 if label_dir== 'neg' else 1)\n",
        "\n",
        "  return texts, labels\n",
        "\n",
        "train_texts, train_labels= read_imdb_split('/content/aclImdb/train')\n",
        "# test_texts, test_labels= read_imdb_split('/content/aclImdb/test')\n",
        "\n",
        "shape=','.join(\n",
        "    [f'train text length: {len(train_texts)}',\n",
        "    f'train labels length: {len(train_labels)}',\n",
        "    # f'test text length: {len(test_texts)}',\n",
        "    # f'test labels length: {len(test_labels)}'\n",
        "    ])\n",
        "\n",
        "print(shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtvE82NrGAp5",
        "outputId": "f9c95151-9a52-4485-a0b6-a01ee4c27661"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train text length: 25000,train labels length: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_texts, train_labels, test_size= 0.2, stratify= train_labels)\n",
        "\n",
        "length= ','. join([\n",
        "    f'X_train length: {len(X_train)}',\n",
        "    f'X_val length: {len(X_val)}',\n",
        "    f'y_train length: {len(y_train)}',\n",
        "    f'y_val length: {len(y_val)}'\n",
        "])\n",
        "\n",
        "print(length)"
      ],
      "metadata": {
        "id": "uFplsiC2JPHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f27ed91c-dad9-4609-c404-530b7aa91701"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train length: 20000,X_val length: 5000,y_train length: 20000,y_val length: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Tokenizer**"
      ],
      "metadata": {
        "id": "RGCp87YjWcDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "check_point= 'FacebookAI/roberta-base'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(check_point)\n",
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkF1ZoWnWfHD",
        "outputId": "f61f60d7-d3ff-4a06-cd2e-43bcf2df7d1f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaTokenizerFast(name_or_path='FacebookAI/roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
              "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
              "}\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(X_train, padding=True, truncation=True, max_length= 256)\n",
        "val_encodings= tokenizer(X_val, padding= True, truncation= True, max_length= 256)\n",
        "# test_encodings= tokenizer(test_texts, padding= True, truncation= True, max_length= 256)"
      ],
      "metadata": {
        "id": "b4GVdYZyWyJB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Make Dataset and DataLoader**"
      ],
      "metadata": {
        "id": "6-tDsfgMeR-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "W-SfaC-BeAnM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings, labels):\n",
        "    self.encodings= encodings\n",
        "    self.labels= labels\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item= {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    item['labels'] = torch.tensor(self.labels[idx])\n",
        "    return item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "\n",
        "train_set= IMDBDataset(train_encodings, y_train)\n",
        "val_set= IMDBDataset(val_encodings, y_val)"
      ],
      "metadata": {
        "id": "K-R68eqbc7Ng"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size= batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size= batch_size)"
      ],
      "metadata": {
        "id": "q1oq-5Wkp9qd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(check_point)\n",
        "model.to(device)\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "VdW420ZP5s-m",
        "outputId": "44d59737-fa71-4e80-89ce-48d8bbc04af2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch =next(iter(train_loader))\n",
        "# batch['labels'][0]\n",
        "# input_ids= batch['input_ids'].to(device)\n",
        "# attention_mask = batch['attention_mask'].to(device)\n",
        "# labels= batch['labels'].to(device)\n",
        "\n",
        "# outputs= model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "# print(outputs.logits)\n",
        "# y_pred = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "# print('y_pred',y_pred)\n",
        "# print('labels',labels)\n",
        "\n",
        "# acc= y_pred == labels\n",
        "# result= torch.sum(y_pred==labels).item()\n",
        "# print(acc)\n",
        "# print(result)"
      ],
      "metadata": {
        "id": "9M3uYvmS6x6o"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "epochs= 10\n",
        "best_loss= float('inf')\n",
        "patience = 3  # Number of epochs to wait for improvement\n",
        "epochs_without_improvement = 0\n",
        "accumulation_steps = 4  # Number of batches to accumulate gradients over\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  total_loss, train_loss, val_loss = 0, 0, 0\n",
        "  total_acc, train_acc, val_acc = 0, 0 , 0\n",
        "\n",
        "  # Set model to training mode\n",
        "  model.train()\n",
        "\n",
        "  optim.zero_grad()   # Clear gradients before starting accumulation\n",
        "  for i, batch in enumerate(train_loader):\n",
        "    # Using GPU\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    y_pred = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = outputs[0]\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "\n",
        "    # Update model parameters after `accumulation_steps` batches\n",
        "    if (i + 1) % accumulation_steps == 0:\n",
        "        optim.step()       # Update model parameters\n",
        "        optim.zero_grad()  # Clear gradients for the next accumulation cycle\n",
        "\n",
        "    # Accumulate loss and accuracy\n",
        "    train_loss += loss.item() * len(batch['input_ids'])\n",
        "    train_acc += torch.sum(y_pred == labels).item()\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "      print(f' batch: [{i + 1}/{math.floor(len(train_set)/batch_size)}]')\n",
        "\n",
        "  # Average loss and accuracy for training\n",
        "  train_loss /= len(train_set)\n",
        "  train_acc /= len(train_set)\n",
        "\n",
        "\n",
        "  # Set model to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "\n",
        "      # Using GPU\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "      y_pred= torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "      # Compuute loss\n",
        "      loss= outputs[0]\n",
        "\n",
        "      # Accumulate loss and accuracy\n",
        "      val_loss += loss.item() * len(batch['input_ids'])\n",
        "      val_acc += torch.sum(y_pred == labels).item()\n",
        "\n",
        "    # Average loss and accuracy for validation\n",
        "    val_loss /= len(val_set)\n",
        "    val_acc /= len(val_set)\n",
        "\n",
        "\n",
        "  # Print training and validation results\n",
        "  training_result= ','.join([\n",
        "    f'Epoch:[{epoch + 1}], '\n",
        "    f'train_Loss:{train_loss:.3f}, '\n",
        "    f'train_acc:{train_acc:.3f}, '\n",
        "    f'val_Loss:{val_loss:.3f}, '\n",
        "    f'val_acc:{val_acc:.3f} '\n",
        "  ])\n",
        "\n",
        "  print(training_result)\n",
        "\n",
        "  # Save the model if validation loss improves\n",
        "  # Early stopping: Added to prevent overfitting.\n",
        "  if val_loss < best_loss:\n",
        "      best_loss = val_loss\n",
        "      epochs_without_improvement = 0\n",
        "      print('model saved!')\n",
        "      torch.save(model, 'imdb_model.pt')\n",
        "  else:\n",
        "      epochs_without_improvement += 1\n",
        "      if epochs_without_improvement >= patience:\n",
        "          print(\"Early stopping!\")\n",
        "          break"
      ],
      "metadata": {
        "id": "efN0NPfSm6Y3",
        "outputId": "fa0de56e-d1bf-4705-bd15-df2b5a048bd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " batch: [10/312]\n",
            " batch: [20/312]\n",
            " batch: [30/312]\n",
            " batch: [40/312]\n",
            " batch: [50/312]\n",
            " batch: [60/312]\n",
            " batch: [70/312]\n",
            " batch: [80/312]\n",
            " batch: [90/312]\n",
            " batch: [100/312]\n",
            " batch: [110/312]\n",
            " batch: [120/312]\n",
            " batch: [130/312]\n",
            " batch: [140/312]\n",
            " batch: [150/312]\n",
            " batch: [160/312]\n",
            " batch: [170/312]\n",
            " batch: [180/312]\n",
            " batch: [190/312]\n",
            " batch: [200/312]\n",
            " batch: [210/312]\n",
            " batch: [220/312]\n",
            " batch: [230/312]\n",
            " batch: [240/312]\n",
            " batch: [250/312]\n",
            " batch: [260/312]\n",
            " batch: [270/312]\n",
            " batch: [280/312]\n",
            " batch: [290/312]\n",
            " batch: [300/312]\n",
            " batch: [310/312]\n",
            "Epoch:[1], train_Loss:0.274, train_acc:0.885, val_Loss:0.187, val_acc:0.929 \n",
            "model saved!\n",
            " batch: [10/312]\n",
            " batch: [20/312]\n",
            " batch: [30/312]\n",
            " batch: [40/312]\n",
            " batch: [50/312]\n",
            " batch: [60/312]\n",
            " batch: [70/312]\n",
            " batch: [80/312]\n",
            " batch: [90/312]\n",
            " batch: [100/312]\n",
            " batch: [110/312]\n",
            " batch: [120/312]\n",
            " batch: [130/312]\n",
            " batch: [140/312]\n",
            " batch: [150/312]\n",
            " batch: [160/312]\n",
            " batch: [170/312]\n",
            " batch: [180/312]\n",
            " batch: [190/312]\n",
            " batch: [200/312]\n",
            " batch: [210/312]\n",
            " batch: [220/312]\n",
            " batch: [230/312]\n",
            " batch: [240/312]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with torch.no_grad():\n",
        "#     outputs = model(input_ids, attention_mask=attention_mask)\n",
        "#     predictions = torch.argmax(outputs.logits, dim=-1)"
      ],
      "metadata": {
        "id": "9GPcA07AvaCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v-AX1zFkCBh-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}