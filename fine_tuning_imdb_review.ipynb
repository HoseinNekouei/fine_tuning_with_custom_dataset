{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMn2NDRqVvup4enSEieIKwL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoseinNekouei/fine_tuning_with_custom_dataset/blob/main/fine_tuning_imdb_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "EQw2NyjwJYR2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7YBDiiuE-Cb"
      },
      "outputs": [],
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the dataset and divide into training and testing set**"
      ],
      "metadata": {
        "id": "gowEYYlAJW3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from posixpath import join\n",
        "from pathlib import Path\n",
        "\n",
        "def read_imdb_split(split_dir):\n",
        "  split_dir= Path(split_dir)\n",
        "  texts=[]\n",
        "  labels=[]\n",
        "\n",
        "  for label_dir in ['neg', 'pos']:\n",
        "    for text_file in (split_dir/label_dir).iterdir():\n",
        "\n",
        "      texts.append(text_file.read_text())\n",
        "      labels.append(0 if label_dir== 'neg' else 1)\n",
        "\n",
        "  return texts, labels\n",
        "\n",
        "train_texts, train_labels= read_imdb_split('/content/aclImdb/train')\n",
        "# test_texts, test_labels= read_imdb_split('/content/aclImdb/test')\n",
        "\n",
        "shape=','.join(\n",
        "    [f'train text length: {len(train_texts)}',\n",
        "    f'train labels length: {len(train_labels)}',\n",
        "    # f'test text length: {len(test_texts)}',\n",
        "    # f'test labels length: {len(test_labels)}'\n",
        "    ])\n",
        "\n",
        "print(shape)"
      ],
      "metadata": {
        "id": "GtvE82NrGAp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_texts, train_labels, test_size= 0.2, stratify= train_labels)\n",
        "\n",
        "length= ','. join([\n",
        "    f'X_train length: {len(X_train)}',\n",
        "    f'X_val length: {len(X_val)}',\n",
        "    f'y_train length: {len(y_train)}',\n",
        "    f'y_val length: {len(y_val)}'\n",
        "])\n",
        "\n",
        "print(length)"
      ],
      "metadata": {
        "id": "uFplsiC2JPHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Tokenizer**"
      ],
      "metadata": {
        "id": "RGCp87YjWcDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased-finetuned-sst-2-english')\n",
        "tokenizer"
      ],
      "metadata": {
        "id": "nkF1ZoWnWfHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(X_train, padding=True, truncation=True, max_length= 256)\n",
        "val_encodings= tokenizer(X_val, padding= True, truncation= True, max_length= 256)\n",
        "# test_encodings= tokenizer(test_texts, padding= True, truncation= True, max_length= 256)"
      ],
      "metadata": {
        "id": "b4GVdYZyWyJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Make Dataset and DataLoader**"
      ],
      "metadata": {
        "id": "6-tDsfgMeR-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "W-SfaC-BeAnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings, labels):\n",
        "    self.encodings= encodings\n",
        "    self.labels= labels\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item= {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    item['labels'] = torch.tensor(self.labels[idx])\n",
        "    return item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "\n",
        "train_set= IMDBDataset(train_encodings, y_train)\n",
        "val_set= IMDBDataset(val_encodings, y_val)"
      ],
      "metadata": {
        "id": "K-R68eqbc7Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=128)"
      ],
      "metadata": {
        "id": "q1oq-5Wkp9qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW, SGD, Adam\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "checkpoint= 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "# optim = AdamW(model.parameters(), lr=5e-5)\n",
        "optim = Adam(model.parameters(), lr=5e-5)\n",
        "# optim = SGD(model.parameters(), lr=0.001,momentum =0.9)"
      ],
      "metadata": {
        "id": "VdW420ZP5s-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch =next(iter(train_loader))\n",
        "input_ids= batch['input_ids'].to(device)\n",
        "attention_mask = batch['attention_mask'].to(device)\n",
        "labels= batch['labels'].to(device)\n",
        "\n",
        "outputs= model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "print(outputs.logits)\n",
        "y_pred = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "print('y_pred',y_pred)\n",
        "print('labels',labels)\n",
        "\n",
        "acc= y_pred == labels\n",
        "result= torch.sum(y_pred==labels).item()\n",
        "print(acc)\n",
        "print(result)\n",
        "\n"
      ],
      "metadata": {
        "id": "9M3uYvmS6x6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Path('.')"
      ],
      "metadata": {
        "id": "CU_-dI3r6adP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "epochs= 3\n",
        "best_loss= float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  total_loss, train_loss, val_loss = 0, 0, 0\n",
        "  total_acc, train_acc, val_acc = 0, 0 , 0\n",
        "\n",
        "  for index, batch in enumerate(train_loader):\n",
        "\n",
        "    # Using GPU\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    # model\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    y_pred = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "    # loss\n",
        "    loss = outputs[0]\n",
        "\n",
        "    # gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "    train_loss += loss.item() * len(batch['input_ids'])\n",
        "    train_acc += torch.sum(y_pred == labels).item()\n",
        "\n",
        "    if (index +1) % 10 == 0:\n",
        "      print(f' batch: [{index + 1}/{math.floor(len(train_set)/128)}]')\n",
        "\n",
        "  train_loss /= len(train_set)\n",
        "  train_acc /= len(train_set)\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "\n",
        "      # GPU\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      # model\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "      y_pred= torch.argmax(outputs.logits, dim=1)\n",
        "      # loss\n",
        "      loss= outputs[0]\n",
        "\n",
        "\n",
        "      val_loss += loss.item() * len(batch['input_ids'])\n",
        "      val_acc += torch.sum(y_pred == labels).item()\n",
        "\n",
        "    val_loss /= len(val_set)\n",
        "    val_acc /= len(val_set)\n",
        "\n",
        "  training_result= ','.join([\n",
        "    f'Epoch:[{epoch + 1}], ',\n",
        "    f'train_Loss:{train_loss:.3f}, ',\n",
        "    f'train_acc:{train_acc:.3f}, '\n",
        "    f'val_Loss:{val_loss:.3f}, ',\n",
        "    f'val_acc:{val_acc:.3f}'\n",
        "  ])\n",
        "\n",
        "  print(training_result)\n",
        "\n",
        "  if val_loss < best_loss:\n",
        "    best_loss= val_loss\n",
        "    print('model saved!')\n",
        "    model.save(model, 'imdb_model.pt')\n"
      ],
      "metadata": {
        "id": "efN0NPfSm6Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     outputs = model(input_ids, attention_mask=attention_mask)\n",
        "#     predictions = torch.argmax(outputs.logits, dim=-1)"
      ],
      "metadata": {
        "id": "9GPcA07AvaCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v-AX1zFkCBh-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}